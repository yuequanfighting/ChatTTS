Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.ckpt\r\n# C extensions\r\n*.so\r\n*.pt\r\n\r\n# Distribution / packaging\r\n.Python\r\noutputs/\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\nshare/python-wheels/\r\n*.egg-info/\r\nasset/*\r\n.installed.cfg\r\n*.egg\r\nMANIFEST\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\ncover/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\n.pybuilder/\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n#   For a library or package, you might want to ignore these files since the code is\r\n#   intended to run in multiple environments; otherwise, check them in:\r\n# .python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# poetry\r\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\r\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\r\n#   commonly ignored for libraries.\r\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\r\n#poetry.lock\r\n\r\n# pdm\r\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\r\n#pdm.lock\r\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\r\n#   in version control.\r\n#   https://pdm.fming.dev/#use-with-ide\r\n.pdm.toml\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n.env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# pytype static type analyzer\r\n.pytype/\r\n\r\n# Cython debug symbols\r\ncython_debug/\r\n\r\n# PyCharm\r\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\r\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\r\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\r\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\r\n#.idea/\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 1c022eeebe577ba3651f4e568fa2dccabaf16e78)
+++ b/.gitignore	(date 1717223914961)
@@ -161,3 +161,7 @@
 #  and can be added to the global gitignore or merged into this file.  For a more nuclear
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
 #.idea/
+/ChatTTS/asset/
+/ChatTTS/config/
+/run.py
+/example_local.ipynb
Index: ChatTTS/core.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nimport os\r\nimport logging\r\nfrom functools import partial\r\nfrom omegaconf import OmegaConf\r\n\r\nimport torch\r\nfrom vocos import Vocos\r\nfrom .model.dvae import DVAE\r\nfrom .model.gpt import GPT_warpper\r\nfrom .utils.gpu_utils import select_device\r\nfrom .utils.infer_utils import count_invalid_characters, detect_language, apply_character_map, apply_half2full_map\r\nfrom .utils.io_utils import get_latest_modified_file\r\nfrom .infer.api import refine_text, infer_code\r\n\r\nfrom huggingface_hub import snapshot_download\r\n\r\nlogging.basicConfig(level = logging.INFO)\r\n\r\n\r\nclass Chat:\r\n    def __init__(self, ):\r\n        self.pretrain_models = {}\r\n        self.normalizer = {}\r\n        self.logger = logging.getLogger(__name__)\r\n        \r\n    def check_model(self, level = logging.INFO, use_decoder = False):\r\n        not_finish = False\r\n        check_list = ['vocos', 'gpt', 'tokenizer']\r\n        \r\n        if use_decoder:\r\n            check_list.append('decoder')\r\n        else:\r\n            check_list.append('dvae')\r\n            \r\n        for module in check_list:\r\n            if module not in self.pretrain_models:\r\n                self.logger.log(logging.WARNING, f'{module} not initialized.')\r\n                not_finish = True\r\n                \r\n        if not not_finish:\r\n            self.logger.log(level, f'All initialized.')\r\n            \r\n        return not not_finish\r\n        \r\n    def load_models(self, source='huggingface', force_redownload=False, local_path='<LOCAL_PATH>', **kwargs):\r\n        if source == 'huggingface':\r\n            hf_home = os.getenv('HF_HOME', os.path.expanduser(\"~/.cache/huggingface\"))\r\n            try:\r\n                download_path = get_latest_modified_file(os.path.join(hf_home, 'hub/models--2Noise--ChatTTS/snapshots'))\r\n            except:\r\n                download_path = None\r\n            if download_path is None or force_redownload: \r\n                self.logger.log(logging.INFO, f'Download from HF: https://huggingface.co/2Noise/ChatTTS')\r\n                download_path = snapshot_download(repo_id=\"2Noise/ChatTTS\", allow_patterns=[\"*.pt\", \"*.yaml\"])\r\n            else:\r\n                self.logger.log(logging.INFO, f'Load from cache: {download_path}')\r\n        elif source == 'local':\r\n            self.logger.log(logging.INFO, f'Load from local: {local_path}')\r\n            download_path = local_path\r\n\r\n        self._load(**{k: os.path.join(download_path, v) for k, v in OmegaConf.load(os.path.join(download_path, 'config', 'path.yaml')).items()}, **kwargs)\r\n        \r\n    def _load(\r\n        self, \r\n        vocos_config_path: str = None, \r\n        vocos_ckpt_path: str = None,\r\n        dvae_config_path: str = None,\r\n        dvae_ckpt_path: str = None,\r\n        gpt_config_path: str = None,\r\n        gpt_ckpt_path: str = None,\r\n        decoder_config_path: str = None,\r\n        decoder_ckpt_path: str = None,\r\n        tokenizer_path: str = None,\r\n        device: str = None,\r\n        compile: bool = True,\r\n    ):\r\n        if not device:\r\n            device = select_device(4096)\r\n            self.logger.log(logging.INFO, f'use {device}')\r\n            \r\n        if vocos_config_path:\r\n            vocos = Vocos.from_hparams(vocos_config_path).to(device).eval()\r\n            assert vocos_ckpt_path, 'vocos_ckpt_path should not be None'\r\n            vocos.load_state_dict(torch.load(vocos_ckpt_path))\r\n            self.pretrain_models['vocos'] = vocos\r\n            self.logger.log(logging.INFO, 'vocos loaded.')\r\n        \r\n        if dvae_config_path:\r\n            cfg = OmegaConf.load(dvae_config_path)\r\n            dvae = DVAE(**cfg).to(device).eval()\r\n            assert dvae_ckpt_path, 'dvae_ckpt_path should not be None'\r\n            dvae.load_state_dict(torch.load(dvae_ckpt_path, map_location='cpu'))\r\n            self.pretrain_models['dvae'] = dvae\r\n            self.logger.log(logging.INFO, 'dvae loaded.')\r\n            \r\n        if gpt_config_path:\r\n            cfg = OmegaConf.load(gpt_config_path)\r\n            gpt = GPT_warpper(**cfg).to(device).eval()\r\n            assert gpt_ckpt_path, 'gpt_ckpt_path should not be None'\r\n            gpt.load_state_dict(torch.load(gpt_ckpt_path, map_location='cpu'))\r\n            if compile and 'cuda' in str(device):\r\n                gpt.gpt.forward = torch.compile(gpt.gpt.forward,  backend='inductor', dynamic=True)\r\n            self.pretrain_models['gpt'] = gpt\r\n            spk_stat_path = os.path.join(os.path.dirname(gpt_ckpt_path), 'spk_stat.pt')\r\n            assert os.path.exists(spk_stat_path), f'Missing spk_stat.pt: {spk_stat_path}'\r\n            self.pretrain_models['spk_stat'] = torch.load(spk_stat_path).to(device)\r\n            self.logger.log(logging.INFO, 'gpt loaded.')\r\n            \r\n        if decoder_config_path:\r\n            cfg = OmegaConf.load(decoder_config_path)\r\n            decoder = DVAE(**cfg).to(device).eval()\r\n            assert decoder_ckpt_path, 'decoder_ckpt_path should not be None'\r\n            decoder.load_state_dict(torch.load(decoder_ckpt_path, map_location='cpu'))\r\n            self.pretrain_models['decoder'] = decoder\r\n            self.logger.log(logging.INFO, 'decoder loaded.')\r\n        \r\n        if tokenizer_path:\r\n            tokenizer = torch.load(tokenizer_path, map_location='cpu')\r\n            tokenizer.padding_side = 'left'\r\n            self.pretrain_models['tokenizer'] = tokenizer\r\n            self.logger.log(logging.INFO, 'tokenizer loaded.')\r\n            \r\n        self.check_model()\r\n    \r\n    def infer(\r\n        self, \r\n        text, \r\n        skip_refine_text=False, \r\n        refine_text_only=False, \r\n        params_refine_text={}, \r\n        params_infer_code={'prompt':'[speed_5]'}, \r\n        use_decoder=True,\r\n        do_text_normalization=True,\r\n        lang=None,\r\n    ):\r\n        \r\n        assert self.check_model(use_decoder=use_decoder)\r\n        \r\n        if not isinstance(text, list): \r\n            text = [text]\r\n        \r\n        if do_text_normalization:\r\n            for i, t in enumerate(text):\r\n                _lang = detect_language(t) if lang is None else lang\r\n                self.init_normalizer(_lang)\r\n                text[i] = self.normalizer[_lang](t)\r\n                if _lang == 'zh':\r\n                    text[i] = apply_half2full_map(text[i])\r\n            \r\n        for i, t in enumerate(text):\r\n            invalid_characters = count_invalid_characters(t)\r\n            if len(invalid_characters):\r\n                self.logger.log(logging.WARNING, f'Invalid characters found! : {invalid_characters}')\r\n                text[i] = apply_character_map(t)\r\n                \r\n        if not skip_refine_text:\r\n            text_tokens = refine_text(self.pretrain_models, text, **params_refine_text)['ids']\r\n            text_tokens = [i[i < self.pretrain_models['tokenizer'].convert_tokens_to_ids('[break_0]')] for i in text_tokens]\r\n            text = self.pretrain_models['tokenizer'].batch_decode(text_tokens)\r\n            if refine_text_only:\r\n                return text\r\n            \r\n        text = [params_infer_code.get('prompt', '') + i for i in text]\r\n        params_infer_code.pop('prompt', '')\r\n        result = infer_code(self.pretrain_models, text, **params_infer_code, return_hidden=use_decoder)\r\n        \r\n        if use_decoder:\r\n            mel_spec = [self.pretrain_models['decoder'](i[None].permute(0,2,1)) for i in result['hiddens']]\r\n        else:\r\n            mel_spec = [self.pretrain_models['dvae'](i[None].permute(0,2,1)) for i in result['ids']]\r\n            \r\n        wav = [self.pretrain_models['vocos'].decode(i).cpu().numpy() for i in mel_spec]\r\n        \r\n        return wav\r\n    \r\n    def sample_random_speaker(self, ):\r\n        \r\n        dim = self.pretrain_models['gpt'].gpt.layers[0].mlp.gate_proj.in_features\r\n        std, mean = self.pretrain_models['spk_stat'].chunk(2)\r\n        return torch.randn(dim, device=std.device) * std + mean\r\n    \r\n    def init_normalizer(self, lang):\r\n        \r\n        if lang not in self.normalizer:\r\n            if lang == 'zh':\r\n                try:\r\n                    from tn.chinese.normalizer import Normalizer\r\n                except:\r\n                    self.logger.log(logging.WARNING, f'Package WeTextProcessing not found! \\\r\n                        Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing')\r\n                self.normalizer[lang] = Normalizer().normalize\r\n            else:\r\n                try:\r\n                    from nemo_text_processing.text_normalization.normalize import Normalizer\r\n                except:\r\n                    self.logger.log(logging.WARNING, f'Package nemo_text_processing not found! \\\r\n                        Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing')\r\n                self.normalizer[lang] = partial(Normalizer(input_case='cased', lang=lang).normalize, verbose=False, punct_post_process=True)\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ChatTTS/core.py b/ChatTTS/core.py
--- a/ChatTTS/core.py	(revision 1c022eeebe577ba3651f4e568fa2dccabaf16e78)
+++ b/ChatTTS/core.py	(date 1717224769735)
@@ -73,7 +73,7 @@
         decoder_ckpt_path: str = None,
         tokenizer_path: str = None,
         device: str = None,
-        compile: bool = True,
+        compile: bool = False,  #windows的话，这个修改为false，否则会报错Windows not yet supported for torch.compile
     ):
         if not device:
             device = select_device(4096)
@@ -131,7 +131,8 @@
         params_refine_text={}, 
         params_infer_code={'prompt':'[speed_5]'}, 
         use_decoder=True,
-        do_text_normalization=True,
+        # do_text_normalization=True,
+        do_text_normalization=False,  ##这里改为False，否则会报错：UnboundLocalError: local variable 'Normalizer' referenced before assignment
         lang=None,
     ):
         
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>omegaconf~=2.3.0\r\ntorch~=2.1.0\r\ntqdm\r\neinops\r\nvector_quantize_pytorch\r\ntransformers~=4.41.1\r\nvocos\r\nIPython
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 1c022eeebe577ba3651f4e568fa2dccabaf16e78)
+++ b/requirements.txt	(date 1717223914974)
@@ -1,8 +1,10 @@
 omegaconf~=2.3.0
-torch~=2.1.0
+#torch~=2.1.0
 tqdm
 einops
 vector_quantize_pytorch
 transformers~=4.41.1
 vocos
-IPython
\ No newline at end of file
+IPython
+#ipykernel
+#ipywidgets
\ No newline at end of file
Index: README_CN.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ChatTTS\r\n[**English**](./README.md) | [**中文简体**](./README_CN.md)\r\n\r\nChatTTS是专门为对话场景设计的文本转语音模型，例如LLM助手对话任务。它支持英文和中文两种语言。最大的模型使用了10万小时以上的中英文数据进行训练。在HuggingFace中开源的版本为4万小时训练且未SFT的版本.\r\n\r\n如需就模型进行正式商业咨询，请发送邮件至 **open-source@2noise.com**。对于中文用户，您可以加入我们的QQ群：808364215 进行讨论。同时欢迎在GitHub上提出问题。如果遇到无法使用 **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** 的情况,可以在 [modelscope](https://www.modelscope.cn/models/pzc163/chatTTS) 上进行下载. \r\n\r\n---\r\n## 亮点\r\n1. **对话式 TTS**: ChatTTS针对对话式任务进行了优化，实现了自然流畅的语音合成，同时支持多说话人。\r\n2. **细粒度控制**: 该模型能够预测和控制细粒度的韵律特征，包括笑声、停顿和插入词等。\r\n3. **更好的韵律**: ChatTTS在韵律方面超越了大部分开源TTS模型。同时提供预训练模型，支持进一步的研究。\r\n\r\n对于模型的具体介绍, 可以参考B站的 **[宣传视频](https://www.bilibili.com/video/BV1zn4y1o7iV)**\r\n\r\n---\r\n\r\n## 免责声明\r\n本文件中的信息仅供学术交流使用。其目的是用于教育和研究，不得用于任何商业或法律目的。作者不保证信息的准确性、完整性或可靠性。本文件中使用的信息和数据，仅用于学术研究目的。这些数据来自公开可用的来源，作者不对数据的所有权或版权提出任何主张。\r\n\r\nChatTTS是一个强大的文本转语音系统。然而，负责任地和符合伦理地利用这项技术是非常重要的。为了限制ChatTTS的使用，我们在4w小时模型的训练过程中添加了少量额外的高频噪音，并用mp3格式尽可能压低了音质，以防不法分子用于潜在的犯罪可能。同时我们在内部训练了检测模型，并计划在未来开放。\r\n\r\n---\r\n## 用法\r\n\r\n<h4>基本用法</h4>\r\n\r\n```python\r\nimport ChatTTS\r\nfrom IPython.display import Audio\r\n\r\nchat = ChatTTS.Chat()\r\nchat.load_models(compile=False) # 设置为True以获得更快速度\r\n\r\ntexts = [\"在这里输入你的文本\",]\r\n\r\nwavs = chat.infer(texts, use_decoder=True)\r\n\r\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\r\n```\r\n\r\n<h4>进阶用法</h4>\r\n\r\n```python\r\n###################################\r\n# Sample a speaker from Gaussian.\r\n\r\nrand_spk = chat.sample_random_speaker()\r\n\r\nparams_infer_code = {\r\n  'spk_emb': rand_spk, # add sampled speaker \r\n  'temperature': .3, # using custom temperature\r\n  'top_P': 0.7, # top P decode\r\n  'top_K': 20, # top K decode\r\n}\r\n\r\n###################################\r\n# For sentence level manual control.\r\n\r\n# use oral_(0-9), laugh_(0-2), break_(0-7) \r\n# to generate special token in text to synthesize.\r\nparams_refine_text = {\r\n  'prompt': '[oral_2][laugh_0][break_6]'\r\n} \r\n\r\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\r\n\r\n###################################\r\n# For word level manual control.\r\n# use_decoder=False to infer faster with a bit worse quality\r\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\r\nwav = chat.infer(text, skip_refine_text=True, params_infer_code=params_infer_code, use_decoder=False)\r\n\r\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\r\n```\r\n\r\n<details open>\r\n  <summary><h4>自我介绍样例</h4></summary>\r\n\r\n```python\r\ninputs_cn = \"\"\"\r\nchat T T S 是一款强大的对话式文本转语音模型。它有中英混读和多说话人的能力。\r\nchat T T S 不仅能够生成自然流畅的语音，还能控制[laugh]笑声啊[laugh]，\r\n停顿啊[uv_break]语气词啊等副语言现象[uv_break]。这个韵律超越了许多开源模型[uv_break]。\r\n请注意，chat T T S 的使用应遵守法律和伦理准则，避免滥用的安全风险。[uv_break]'\r\n\"\"\".replace('\\n', '')\r\n\r\nparams_refine_text = {\r\n  'prompt': '[oral_2][laugh_0][break_4]'\r\n} \r\naudio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\r\n# audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\r\n\r\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_cn[0]), 24000)\r\n```\r\n[男说话人](https://github.com/2noise/ChatTTS/assets/130631963/bbfa3b83-2b67-4bb6-9315-64c992b63788)\r\n\r\n[女说话人](https://github.com/2noise/ChatTTS/assets/130631963/e061f230-0e05-45e6-8e4e-0189f2d260c4)\r\n</details>\r\n\r\n\r\n---\r\n## 计划路线\r\n- [x] 开源4w小时基础模型和spk_stats文件\r\n- [ ] 开源VQ encoder和Lora 训练代码\r\n- [ ] 在非refine text情况下, 流式生成音频*\r\n- [ ] 开源多情感可控的4w小时版本\r\n- [ ] ChatTTS.cpp maybe? (欢迎社区PR或独立的新repo)\r\n\r\n---\r\n## 常见问题\r\n\r\n##### 连不上HuggingFace\r\n请使用[modelscope](https://www.modelscope.cn/models/pzc163/chatTTS)的版本. 并设置cache的位置:\r\n```python\r\nchat.load_models(source='local', local_path='你的下载位置')\r\n```\r\n\r\n##### 我要多少显存? Infer的速度是怎么样的?\r\n对于30s的音频, 至少需要4G的显存. 对于4090, 1s生成约7个字所对应的音频. RTF约0.3.\r\n\r\n##### 模型稳定性似乎不够好, 会出现其他说话人或音质很差的现象.\r\n这是自回归模型通常都会出现的问题. 说话人可能会在中间变化, 可能会采样到音质非常差的结果, 这通常难以避免. 可以多采样几次来找到合适的结果.\r\n\r\n##### 除了笑声还能控制什么吗? 还能控制其他情感吗?\r\n在现在放出的模型版本中, 只有[laugh]和[uv_break], [lbreak]作为字级别的控制单元. 在未来的版本中我们可能会开源其他情感控制的版本.\r\n\r\n---\r\n## 致谢\r\n- [bark](https://github.com/suno-ai/bark),[XTTSv2](https://github.com/coqui-ai/TTS)和[valle](https://arxiv.org/abs/2301.02111)展示了自回归任务用于TTS任务的可能性.\r\n- [fish-speech](https://github.com/fishaudio/fish-speech)一个优秀的自回归TTS模型, 揭示了GVQ用于LLM任务的可能性.\r\n- [vocos](https://github.com/gemelo-ai/vocos)作为模型中的vocoder.\r\n\r\n---\r\n## 特别致谢\r\n- [wlu-audio lab](https://audio.westlake.edu.cn/)为我们提供了早期算法试验的支持.\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README_CN.md b/README_CN.md
--- a/README_CN.md	(revision 1c022eeebe577ba3651f4e568fa2dccabaf16e78)
+++ b/README_CN.md	(date 1717223914977)
@@ -1,3 +1,6 @@
+# 前言
+## 目的
+这个仓库是对[ChatTTS](https://github.com/2noise/ChatTTS?ref=upstract.com)的一个补充，具体来说是关于Colab，Windows的一些详细部署说明。
 # ChatTTS
 [**English**](./README.md) | [**中文简体**](./README_CN.md)
 
