Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.ckpt\r\n# C extensions\r\n*.so\r\n*.pt\r\n\r\n# Distribution / packaging\r\n.Python\r\noutputs/\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\nshare/python-wheels/\r\n*.egg-info/\r\nasset/*\r\n.installed.cfg\r\n*.egg\r\nMANIFEST\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\ncover/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\n.pybuilder/\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n#   For a library or package, you might want to ignore these files since the code is\r\n#   intended to run in multiple environments; otherwise, check them in:\r\n# .python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# poetry\r\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\r\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\r\n#   commonly ignored for libraries.\r\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\r\n#poetry.lock\r\n\r\n# pdm\r\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\r\n#pdm.lock\r\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\r\n#   in version control.\r\n#   https://pdm.fming.dev/#use-with-ide\r\n.pdm.toml\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n.env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# pytype static type analyzer\r\n.pytype/\r\n\r\n# Cython debug symbols\r\ncython_debug/\r\n\r\n# PyCharm\r\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\r\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\r\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\r\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\r\n#.idea/\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	
+++ b/.gitignore	
@@ -161,3 +161,4 @@
 #  and can be added to the global gitignore or merged into this file.  For a more nuclear
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
 #.idea/
+/自用.ipynb
Index: ChatTTS/core.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nimport os\r\nimport logging\r\nfrom functools import partial\r\nfrom omegaconf import OmegaConf\r\n\r\nimport torch\r\nfrom vocos import Vocos\r\nfrom .model.dvae import DVAE\r\nfrom .model.gpt import GPT_warpper\r\nfrom .utils.gpu_utils import select_device\r\nfrom .utils.infer_utils import count_invalid_characters, detect_language, apply_character_map, apply_half2full_map\r\nfrom .utils.io_utils import get_latest_modified_file\r\nfrom .infer.api import refine_text, infer_code\r\n\r\nfrom huggingface_hub import snapshot_download\r\n\r\nlogging.basicConfig(level = logging.INFO)\r\n\r\n\r\nclass Chat:\r\n    def __init__(self, ):\r\n        self.pretrain_models = {}\r\n        self.normalizer = {}\r\n        self.logger = logging.getLogger(__name__)\r\n        \r\n    def check_model(self, level = logging.INFO, use_decoder = False):\r\n        not_finish = False\r\n        check_list = ['vocos', 'gpt', 'tokenizer']\r\n        \r\n        if use_decoder:\r\n            check_list.append('decoder')\r\n        else:\r\n            check_list.append('dvae')\r\n            \r\n        for module in check_list:\r\n            if module not in self.pretrain_models:\r\n                self.logger.log(logging.WARNING, f'{module} not initialized.')\r\n                not_finish = True\r\n                \r\n        if not not_finish:\r\n            self.logger.log(level, f'All initialized.')\r\n            \r\n        return not not_finish\r\n        \r\n    def load_models(self, source='huggingface', force_redownload=False, local_path='<LOCAL_PATH>', **kwargs):\r\n        if source == 'huggingface':\r\n            hf_home = os.getenv('HF_HOME', os.path.expanduser(\"~/.cache/huggingface\"))\r\n            try:\r\n                download_path = get_latest_modified_file(os.path.join(hf_home, 'hub/models--2Noise--ChatTTS/snapshots'))\r\n            except:\r\n                download_path = None\r\n            if download_path is None or force_redownload: \r\n                self.logger.log(logging.INFO, f'Download from HF: https://huggingface.co/2Noise/ChatTTS')\r\n                download_path = snapshot_download(repo_id=\"2Noise/ChatTTS\", allow_patterns=[\"*.pt\", \"*.yaml\"])\r\n            else:\r\n                self.logger.log(logging.INFO, f'Load from cache: {download_path}')\r\n        elif source == 'local':\r\n            self.logger.log(logging.INFO, f'Load from local: {local_path}')\r\n            download_path = local_path\r\n\r\n        self._load(**{k: os.path.join(download_path, v) for k, v in OmegaConf.load(os.path.join(download_path, 'config', 'path.yaml')).items()}, **kwargs)\r\n        \r\n    def _load(\r\n        self, \r\n        vocos_config_path: str = None, \r\n        vocos_ckpt_path: str = None,\r\n        dvae_config_path: str = None,\r\n        dvae_ckpt_path: str = None,\r\n        gpt_config_path: str = None,\r\n        gpt_ckpt_path: str = None,\r\n        decoder_config_path: str = None,\r\n        decoder_ckpt_path: str = None,\r\n        tokenizer_path: str = None,\r\n        device: str = None,\r\n        compile: bool = True,\r\n    ):\r\n        if not device:\r\n            device = select_device(4096)\r\n            self.logger.log(logging.INFO, f'use {device}')\r\n            \r\n        if vocos_config_path:\r\n            vocos = Vocos.from_hparams(vocos_config_path).to(device).eval()\r\n            assert vocos_ckpt_path, 'vocos_ckpt_path should not be None'\r\n            vocos.load_state_dict(torch.load(vocos_ckpt_path))\r\n            self.pretrain_models['vocos'] = vocos\r\n            self.logger.log(logging.INFO, 'vocos loaded.')\r\n        \r\n        if dvae_config_path:\r\n            cfg = OmegaConf.load(dvae_config_path)\r\n            dvae = DVAE(**cfg).to(device).eval()\r\n            assert dvae_ckpt_path, 'dvae_ckpt_path should not be None'\r\n            dvae.load_state_dict(torch.load(dvae_ckpt_path, map_location='cpu'))\r\n            self.pretrain_models['dvae'] = dvae\r\n            self.logger.log(logging.INFO, 'dvae loaded.')\r\n            \r\n        if gpt_config_path:\r\n            cfg = OmegaConf.load(gpt_config_path)\r\n            gpt = GPT_warpper(**cfg).to(device).eval()\r\n            assert gpt_ckpt_path, 'gpt_ckpt_path should not be None'\r\n            gpt.load_state_dict(torch.load(gpt_ckpt_path, map_location='cpu'))\r\n            if compile and 'cuda' in str(device):\r\n                gpt.gpt.forward = torch.compile(gpt.gpt.forward,  backend='inductor', dynamic=True)\r\n            self.pretrain_models['gpt'] = gpt\r\n            spk_stat_path = os.path.join(os.path.dirname(gpt_ckpt_path), 'spk_stat.pt')\r\n            assert os.path.exists(spk_stat_path), f'Missing spk_stat.pt: {spk_stat_path}'\r\n            self.pretrain_models['spk_stat'] = torch.load(spk_stat_path).to(device)\r\n            self.logger.log(logging.INFO, 'gpt loaded.')\r\n            \r\n        if decoder_config_path:\r\n            cfg = OmegaConf.load(decoder_config_path)\r\n            decoder = DVAE(**cfg).to(device).eval()\r\n            assert decoder_ckpt_path, 'decoder_ckpt_path should not be None'\r\n            decoder.load_state_dict(torch.load(decoder_ckpt_path, map_location='cpu'))\r\n            self.pretrain_models['decoder'] = decoder\r\n            self.logger.log(logging.INFO, 'decoder loaded.')\r\n        \r\n        if tokenizer_path:\r\n            tokenizer = torch.load(tokenizer_path, map_location='cpu')\r\n            tokenizer.padding_side = 'left'\r\n            self.pretrain_models['tokenizer'] = tokenizer\r\n            self.logger.log(logging.INFO, 'tokenizer loaded.')\r\n            \r\n        self.check_model()\r\n    \r\n    def infer(\r\n        self, \r\n        text, \r\n        skip_refine_text=False, \r\n        refine_text_only=False, \r\n        params_refine_text={}, \r\n        params_infer_code={'prompt':'[speed_5]'}, \r\n        use_decoder=True,\r\n        do_text_normalization=True,\r\n        lang=None,\r\n    ):\r\n        \r\n        assert self.check_model(use_decoder=use_decoder)\r\n        \r\n        if not isinstance(text, list): \r\n            text = [text]\r\n        \r\n        if do_text_normalization:\r\n            for i, t in enumerate(text):\r\n                _lang = detect_language(t) if lang is None else lang\r\n                self.init_normalizer(_lang)\r\n                text[i] = self.normalizer[_lang](t)\r\n                if _lang == 'zh':\r\n                    text[i] = apply_half2full_map(text[i])\r\n            \r\n        for i, t in enumerate(text):\r\n            invalid_characters = count_invalid_characters(t)\r\n            if len(invalid_characters):\r\n                self.logger.log(logging.WARNING, f'Invalid characters found! : {invalid_characters}')\r\n                text[i] = apply_character_map(t)\r\n                \r\n        if not skip_refine_text:\r\n            text_tokens = refine_text(self.pretrain_models, text, **params_refine_text)['ids']\r\n            text_tokens = [i[i < self.pretrain_models['tokenizer'].convert_tokens_to_ids('[break_0]')] for i in text_tokens]\r\n            text = self.pretrain_models['tokenizer'].batch_decode(text_tokens)\r\n            if refine_text_only:\r\n                return text\r\n            \r\n        text = [params_infer_code.get('prompt', '') + i for i in text]\r\n        params_infer_code.pop('prompt', '')\r\n        result = infer_code(self.pretrain_models, text, **params_infer_code, return_hidden=use_decoder)\r\n        \r\n        if use_decoder:\r\n            mel_spec = [self.pretrain_models['decoder'](i[None].permute(0,2,1)) for i in result['hiddens']]\r\n        else:\r\n            mel_spec = [self.pretrain_models['dvae'](i[None].permute(0,2,1)) for i in result['ids']]\r\n            \r\n        wav = [self.pretrain_models['vocos'].decode(i).cpu().numpy() for i in mel_spec]\r\n        \r\n        return wav\r\n    \r\n    def sample_random_speaker(self, ):\r\n        \r\n        dim = self.pretrain_models['gpt'].gpt.layers[0].mlp.gate_proj.in_features\r\n        std, mean = self.pretrain_models['spk_stat'].chunk(2)\r\n        return torch.randn(dim, device=std.device) * std + mean\r\n    \r\n    def init_normalizer(self, lang):\r\n        \r\n        if lang not in self.normalizer:\r\n            if lang == 'zh':\r\n                try:\r\n                    from tn.chinese.normalizer import Normalizer\r\n                except:\r\n                    self.logger.log(logging.WARNING, f'Package WeTextProcessing not found! \\\r\n                        Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing')\r\n                self.normalizer[lang] = Normalizer().normalize\r\n            else:\r\n                try:\r\n                    from nemo_text_processing.text_normalization.normalize import Normalizer\r\n                except:\r\n                    self.logger.log(logging.WARNING, f'Package nemo_text_processing not found! \\\r\n                        Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing')\r\n                self.normalizer[lang] = partial(Normalizer(input_case='cased', lang=lang).normalize, verbose=False, punct_post_process=True)\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ChatTTS/core.py b/ChatTTS/core.py
--- a/ChatTTS/core.py	
+++ b/ChatTTS/core.py	
@@ -73,7 +73,7 @@
         decoder_ckpt_path: str = None,
         tokenizer_path: str = None,
         device: str = None,
-        compile: bool = True,
+        compile: bool = False,  #windows的话，这个修改为false，否则会报错Windows not yet supported for torch.compile
     ):
         if not device:
             device = select_device(4096)
@@ -131,7 +131,8 @@
         params_refine_text={}, 
         params_infer_code={'prompt':'[speed_5]'}, 
         use_decoder=True,
-        do_text_normalization=True,
+        # do_text_normalization=True,
+        do_text_normalization=False,  ##这里改为False，否则会报错：UnboundLocalError: local variable 'Normalizer' referenced before assignment
         lang=None,
     ):
         
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>omegaconf~=2.3.0\r\ntorch~=2.1.0\r\ntqdm\r\neinops\r\nvector_quantize_pytorch\r\ntransformers~=4.41.1\r\nvocos\r\nIPython
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	
+++ b/requirements.txt	
@@ -1,8 +1,9 @@
 omegaconf~=2.3.0
-torch~=2.1.0
+torch~=2.1.0  #我这里安装的是2.1.2，版本不要太高或者太低，否则可能会报错。
 tqdm
 einops
 vector_quantize_pytorch
 transformers~=4.41.1
 vocos
-IPython
\ No newline at end of file
+IPython
+#本地运行需要自行安装Jupyter
Index: ChatTTS/experimental/llm.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom openai import OpenAI\r\n \r\nprompt_dict = {\r\n    'kimi': [ {\"role\": \"system\", \"content\": \"你是 Kimi，由 Moonshot AI 提供的人工智能助手，你更擅长中文和英文的对话。\"},\r\n              {\"role\": \"user\", \"content\": \"你好，请注意你现在生成的文字要按照人日常生活的口吻，你的回复将会后续用TTS模型转为语音，并且请把回答控制在100字以内。并且标点符号仅包含逗号和句号，将数字等转为文字回答。\"},\r\n              {\"role\": \"assistant\", \"content\": \"好的，我现在生成的文字将按照人日常生活的口吻， 并且我会把回答控制在一百字以内, 标点符号仅包含逗号和句号，将阿拉伯数字等转为中文文字回答。下面请开始对话。\"},],\r\n    'deepseek': [\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n        {\"role\": \"user\", \"content\": \"你好，请注意你现在生成的文字要按照人日常生活的口吻，你的回复将会后续用TTS模型转为语音，并且请把回答控制在100字以内。并且标点符号仅包含逗号和句号，将数字等转为文字回答。\"},\r\n        {\"role\": \"assistant\", \"content\": \"好的，我现在生成的文字将按照人日常生活的口吻， 并且我会把回答控制在一百字以内, 标点符号仅包含逗号和句号，将阿拉伯数字等转为中文文字回答。下面请开始对话。\"},],\r\n    'deepseek_TN': [\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n        {\"role\": \"user\", \"content\": \"你好，现在我们在处理TTS的文本输入，下面将会给你输入一段文本，请你将其中的阿拉伯数字等等转为文字表达，并且输出的文本里仅包含逗号和句号这两个标点符号\"},\r\n        {\"role\": \"assistant\", \"content\": \"好的，我现在对TTS的文本输入进行处理。这一般叫做text normalization。下面请输入\"},\r\n        {\"role\": \"user\", \"content\": \"We paid $123 for this desk.\"},\r\n        {\"role\": \"assistant\", \"content\": \"We paid one hundred and twenty three dollars for this desk.\"},\r\n        {\"role\": \"user\", \"content\": \"详询请拨打010-724654\"},\r\n        {\"role\": \"assistant\", \"content\": \"详询请拨打零幺零，七二四六五四\"},\r\n        {\"role\": \"user\", \"content\": \"罗森宣布将于7月24日退市，在华门店超6000家！\"},\r\n        {\"role\": \"assistant\", \"content\": \"罗森宣布将于七月二十四日退市，在华门店超过六千家。\"},\r\n        ],\r\n}          \r\n                \r\nclass llm_api:\r\n    def __init__(self, api_key, base_url, model):\r\n        self.client =  OpenAI(\r\n            api_key = api_key,\r\n            base_url = base_url,\r\n        )\r\n        self.model = model\r\n    def call(self, user_question, temperature = 0.3, prompt_version='kimi', **kwargs):\r\n    \r\n        completion = self.client.chat.completions.create(\r\n            model = self.model,\r\n            messages = prompt_dict[prompt_version]+[{\"role\": \"user\", \"content\": user_question},],\r\n            temperature = temperature,\r\n            **kwargs\r\n        )\r\n        return completion.choices[0].message.content\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ChatTTS/experimental/llm.py b/ChatTTS/experimental/llm.py
--- a/ChatTTS/experimental/llm.py	
+++ b/ChatTTS/experimental/llm.py	
@@ -20,6 +20,7 @@
         {"role": "user", "content": "罗森宣布将于7月24日退市，在华门店超6000家！"},
         {"role": "assistant", "content": "罗森宣布将于七月二十四日退市，在华门店超过六千家。"},
         ],
+
 }          
                 
 class llm_api:
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ChatTTS\r\n[**English**](./README.md) | [**中文简体**](./README_CN.md)\r\n\r\nChatTTS is a text-to-speech model designed specifically for dialogue scenario such as LLM assistant. It supports both English and Chinese languages. Our model is trained with 100,000+ hours composed of chinese and english. The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre trained model without SFT.\r\n\r\nFor formal inquiries about model and roadmap, please contact us at **open-source@2noise.com**. You could join our QQ group: 808364215 for discussion. Adding github issues is always welcomed.\r\n\r\n---\r\n## Highlights\r\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\r\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \r\n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\r\n\r\nFor the detailed description of the model, you can refer to **[video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**\r\n\r\n---\r\n\r\n## Disclaimer\r\n\r\nThis repo is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\r\n\r\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\r\n\r\n\r\n---\r\n## Usage\r\n\r\n<h4>Basic usage</h4>\r\n\r\n```python\r\nimport ChatTTS\r\nfrom IPython.display import Audio\r\n\r\nchat = ChatTTS.Chat()\r\nchat.load_models(compile=False) # Set to True for better performance\r\n\r\ntexts = [\"PUT YOUR TEXT HERE\",]\r\n\r\nwavs = chat.infer(texts, )\r\n\r\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)\r\n```\r\n\r\n<h4>Advanced usage</h4>\r\n\r\n```python\r\n###################################\r\n# Sample a speaker from Gaussian.\r\n\r\nrand_spk = chat.sample_random_speaker()\r\n\r\nparams_infer_code = {\r\n  'spk_emb': rand_spk, # add sampled speaker \r\n  'temperature': .3, # using custom temperature\r\n  'top_P': 0.7, # top P decode\r\n  'top_K': 20, # top K decode\r\n}\r\n\r\n###################################\r\n# For sentence level manual control.\r\n\r\n# use oral_(0-9), laugh_(0-2), break_(0-7) \r\n# to generate special token in text to synthesize.\r\nparams_refine_text = {\r\n  'prompt': '[oral_2][laugh_0][break_6]'\r\n} \r\n\r\nwav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)\r\n\r\n###################################\r\n# For word level manual control.\r\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\r\nwav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\r\ntorchaudio.save(\"output2.wav\", torch.from_numpy(wavs[0]), 24000)\r\n```\r\n\r\n<details open>\r\n  <summary><h4>Example: self introduction</h4></summary>\r\n\r\n```python\r\ninputs_en = \"\"\"\r\nchat T T S is a text to speech model designed for dialogue applications. \r\n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \r\ncapabilities with precise control over prosodic elements [laugh]like like \r\n[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. \r\n[uv_break]it delivers natural and expressive speech,[uv_break]so please\r\n[uv_break] use the project responsibly at your own risk.[uv_break]\r\n\"\"\".replace('\\n', '') # English is still experimental.\r\n\r\nparams_refine_text = {\r\n  'prompt': '[oral_2][laugh_0][break_4]'\r\n} \r\n# audio_array_cn = chat.infer(inputs_cn, params_refine_text=params_refine_text)\r\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\r\ntorchaudio.save(\"output3.wav\", torch.from_numpy(audio_array_en[0]), 24000)\r\n```\r\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\r\n\r\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\r\n</details>\r\n\r\n---\r\n## Roadmap\r\n- [x] Open-source the 40k hour base model and spk_stats file\r\n- [ ] Open-source VQ encoder and Lora training code\r\n- [ ] Streaming audio generation without refining the text*\r\n- [ ] Open-source the 40k hour version with multi-emotion control\r\n- [ ] ChatTTS.cpp maybe? (PR or new repo are welcomed.)\r\n \r\n----\r\n## FAQ\r\n\r\n##### How much VRAM do I need? How about infer speed?\r\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\r\n\r\n##### model stability is not good enough, with issues such as multi speakers or poor audio quality.\r\n\r\nThis is a problem that typically occurs with autoregressive models(for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\r\n\r\n##### Besides laughter, can we control anything else? Can we control other emotions?\r\n\r\nIn the current released model, the only token-level control units are [laugh], [uv_break], and [lbreak]. In future versions, we may open-source models with additional emotional control capabilities.\r\n\r\n---\r\n## Acknowledgements\r\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demostrate a remarkable TTS result by a autoregressive-style system.\r\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\r\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\r\n\r\n---\r\n## Special Appreciation\r\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	
+++ b/README.md	
@@ -1,3 +1,13 @@
+# 前述
+- 项目来自[2noise团队](https://github.com/2noise/ChatTTS)，我这里对Windows和Colab进行了部署。
+- Colab链接为[Colab](https://colab.research.google.com/drive/1bNGByb7aCl7o8NyPSWbZpYGZmqj9B4s3?usp=sharing),
+- 如果没有过环境部署经验的，不建议用我这个仓库部署到Windows
+- Windows一键部署，参考：https://github.com/cronrpc/ChatTTS-webui 
+[视频教程](https://www.bilibili.com/video/BV1rn4y1o7Qu/?spm_id_from=..search-card.all.click&vd_source=e7890b2ee44c6eed1236ac02821d8d0e)
+- Macos整合包参考：[视频教程](https://www.bilibili.com/video/BV13n4y197ic/?spm_id_from=..search-card.all.click)
+
+
+
 # ChatTTS
 [**English**](./README.md) | [**中文简体**](./README_CN.md)
 
